{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Series wise Topic Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to replace the chapter names with the correct chapter order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same function in test_eda.ipynb so need to put this in one place in the the scripts\n",
    "def chapter_order(filename):\n",
    "\n",
    "    book_number = filename.split('_')[0]\n",
    "    chapter_number = filename.split('_')[2].zfill(2)\n",
    "    new_name = book_number + '.' + chapter_number\n",
    "    \n",
    "    return new_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Chapter name and text dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all chapters to one list of chapter text\n",
    "# one element of the list = one chapter\n",
    "# Just like text_sentiment_analysis.ipynb\n",
    "\n",
    "mb_dir = '../2_Text_Preprocessing/TMBD_Chapters_lemma'\n",
    "\n",
    "mb_chapter_texts = {}\n",
    "\n",
    "for filename in os.listdir(mb_dir):\n",
    "    filepath =  os.path.join(mb_dir, filename)\n",
    "    with open(filepath, 'r', encoding='utf-8') as chapter_file:\n",
    "        chapter_text = chapter_file.read()\n",
    "        mb_chapter_texts[chapter_order(filename)] = chapter_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all chapters to one list of chapter text\n",
    "# one element of the list = one chapter\n",
    "# same as test_sentiment_analysis.ipynb\n",
    "\n",
    "ir_dir = '../2_Text_Preprocessing/IR_Chapters_lemma'\n",
    "\n",
    "ir_chapter_texts = {}\n",
    "\n",
    "for filename in os.listdir(ir_dir):\n",
    "    filepath =  os.path.join(ir_dir, filename)\n",
    "    with open(filepath, 'r', encoding='utf-8') as chapter_file:\n",
    "        chapter_text = chapter_file.read()\n",
    "        ir_chapter_texts[chapter_order(filename)] = chapter_text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have both dictionaries with chapter names as keys and chapter text as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_corpus_dict = corpora.Dictionary([values for key, values in mb_chapter_texts.items()])\n",
    "mb_corpus = [mb_corpus_dict.doc2bow(values) for key, values in mb_chapter_texts.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ir_corpus_dict = corpora.Dictionary([values for key, values in ir_chapter_texts.items()])\n",
    "ir_corpus = [ir_corpus_dict.doc2bow(values) for key, values in ir_chapter_texts.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.014*\"said\" + 0.008*\"humans\" + 0.008*\"artdrone\" + 0.006*\"ratthi\" + 0.006*\"think\" + 0.006*\"know\"')\n",
      "(1, '0.008*\"humans\" + 0.008*\"security\" + 0.008*\"would\" + 0.008*\"feed\" + 0.007*\"one\" + 0.007*\"didnt\"')\n",
      "(2, '0.013*\"said\" + 0.009*\"would\" + 0.008*\"humans\" + 0.008*\"could\" + 0.007*\"feed\" + 0.007*\"miki\"')\n",
      "(3, '0.000*\"said\" + 0.000*\"would\" + 0.000*\"didnt\" + 0.000*\"could\" + 0.000*\"art\" + 0.000*\"like\"')\n",
      "(4, '0.016*\"station\" + 0.015*\"indah\" + 0.010*\"said\" + 0.009*\"aylen\" + 0.007*\"could\" + 0.007*\"security\"')\n",
      "(5, '0.008*\"secunit\" + 0.008*\"targetcontrolsystem\" + 0.007*\"said\" + 0.006*\"humans\" + 0.005*\"3\" + 0.005*\"adacol1\"')\n",
      "(6, '0.015*\"art\" + 0.014*\"said\" + 0.008*\"humans\" + 0.007*\"could\" + 0.007*\"like\" + 0.007*\"know\"')\n",
      "(7, '0.013*\"said\" + 0.008*\"one\" + 0.007*\"didnt\" + 0.007*\"feed\" + 0.007*\"could\" + 0.007*\"humans\"')\n",
      "(8, '0.000*\"said\" + 0.000*\"like\" + 0.000*\"humans\" + 0.000*\"one\" + 0.000*\"could\" + 0.000*\"would\"')\n",
      "(9, '0.006*\"iris\" + 0.005*\"thiago\" + 0.005*\"chamber\" + 0.005*\"targets\" + 0.005*\"humans\" + 0.005*\"overse\"')\n"
     ]
    }
   ],
   "source": [
    "mb_lda_model = LdaModel(corpus=mb_corpus, id2word=mb_corpus_dict, num_topics=10, random_state=42, passes=15)\n",
    "\n",
    "mb_topics = mb_lda_model.print_topics(num_words=6)\n",
    "\n",
    "for topic in mb_topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to remove \"said\", \"would\", \"could\" as stop words for the MB series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.018*\"sword\" + 0.011*\"seivarden\" + 0.010*\"said\" + 0.010*\"station\" + 0.009*\"tisarwat\" + 0.009*\"atagaris\"')\n",
      "(1, '0.014*\"would\" + 0.014*\"lieutenant\" + 0.013*\"one\" + 0.012*\"said\" + 0.009*\"mianaai\" + 0.009*\"awn\"')\n",
      "(2, '0.013*\"said\" + 0.007*\"one\" + 0.006*\"captain\" + 0.006*\"could\" + 0.006*\"would\" + 0.005*\"didnt\"')\n",
      "(3, '0.010*\"would\" + 0.008*\"said\" + 0.008*\"strigan\" + 0.007*\"seivarden\" + 0.007*\"one\" + 0.007*\"could\"')\n",
      "(4, '0.000*\"said\" + 0.000*\"would\" + 0.000*\"one\" + 0.000*\"seivarden\" + 0.000*\"lieutenant\" + 0.000*\"could\"')\n",
      "(5, '0.014*\"said\" + 0.010*\"would\" + 0.009*\"captain\" + 0.009*\"station\" + 0.008*\"one\" + 0.008*\"seivarden\"')\n",
      "(6, '0.015*\"said\" + 0.012*\"lieutenant\" + 0.010*\"would\" + 0.010*\"one\" + 0.007*\"could\" + 0.006*\"awn\"')\n",
      "(7, '0.000*\"would\" + 0.000*\"said\" + 0.000*\"one\" + 0.000*\"captain\" + 0.000*\"lieutenant\" + 0.000*\"seivarden\"')\n",
      "(8, '0.028*\"lieutenant\" + 0.012*\"awn\" + 0.011*\"said\" + 0.011*\"one\" + 0.007*\"esk\" + 0.006*\"would\"')\n",
      "(9, '0.000*\"said\" + 0.000*\"would\" + 0.000*\"lieutenant\" + 0.000*\"one\" + 0.000*\"seivarden\" + 0.000*\"station\"')\n"
     ]
    }
   ],
   "source": [
    "ir_lda_model = LdaModel(corpus=ir_corpus, id2word=ir_corpus_dict, num_topics=10, random_state=42, passes=15)\n",
    "\n",
    "ir_topics = ir_lda_model.print_topics(num_words=6)\n",
    "\n",
    "for topic in ir_topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to remove \"said\", \"would\", \"could\", \"didn't\" as stop words for the IR series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Series wise TF-IDF for Theme Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn. feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "mb_tfidf_matrix = mb_vectorizer.fit_transform([\" \".join(values) for key, values in mb_chapter_texts.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "said\n",
      "humans\n",
      "didnt\n",
      "feed\n",
      "art\n",
      "like\n",
      "know\n",
      "ratthi\n",
      "mensah\n",
      "human\n"
     ]
    }
   ],
   "source": [
    "mb_feature_names = mb_vectorizer.get_feature_names_out()\n",
    "\n",
    "mb_tfidf_sums = mb_tfidf_matrix.sum(axis=0)\n",
    "mb_sorted_indices = mb_tfidf_sums.argsort()[0, ::-1]\n",
    "\n",
    "mb_top_words = [mb_feature_names[i] for i in mb_sorted_indices[:10]]\n",
    "for word in mb_top_words[0][0][:10]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ir_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "ir_tfidf_matrix = ir_vectorizer.fit_transform([\" \".join(values) for key, values in ir_chapter_texts.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "said\n",
      "lieutenant\n",
      "seivarden\n",
      "captain\n",
      "station\n",
      "tisarwat\n",
      "didnt\n",
      "fleet\n",
      "ship\n",
      "know\n"
     ]
    }
   ],
   "source": [
    "ir_feature_names = ir_vectorizer.get_feature_names_out()\n",
    "\n",
    "ir_tfidf_sums = ir_tfidf_matrix.sum(axis=0)\n",
    "ir_sorted_indices = ir_tfidf_sums.argsort()[0, ::-1]\n",
    "\n",
    "ir_top_words = [ir_feature_names[i] for i in ir_sorted_indices[:10]]\n",
    "for word in ir_top_words[0][0][:10]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So not much difference from the word frequencies in test_eda.ipynb\n",
    "\n",
    "- both texts have \"said\" as a theme which makes sense for books made of a lot of dialogue or any kind of written fiction.\n",
    "\n",
    "- in MB ratthi is a topic higher than mensah, which is interesting. ART is also higher up and humans still are a central theme. despite this series being about machine intelligences, the focus is entirely human. interesting.\n",
    "\n",
    "- in IR series, the titles are still as important and what i take to be main characters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'could', 'didnt', 'dont', 'get', 'know', 'like', 'one', 'said', 'would'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_stop_words = set([\"said\", \"would\", \"could\", \"didnt\", \"like\", \"know\", \"one\", \"get\", \"dont\"])\n",
    "new_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreating the mb_chapter_texts dictionary\n",
    "\n",
    "mb_new_chapter_texts = {}\n",
    "\n",
    "for filename in os.listdir(mb_dir):\n",
    "    filepath =  os.path.join(mb_dir, filename)\n",
    "    with open(filepath, 'r', encoding='utf-8') as chapter_file:\n",
    "        chapter_text = chapter_file.read()\n",
    "        mb_new_chapter_texts[chapter_order(filename)] = [word for word in chapter_text.split() if word not in new_stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreating the ir_chapter_texts dictionary\n",
    "\n",
    "ir_new_chapter_texts = {}\n",
    "\n",
    "for filename in os.listdir(ir_dir):\n",
    "    filepath =  os.path.join(ir_dir, filename)\n",
    "    with open(filepath, 'r', encoding='utf-8') as chapter_file:\n",
    "        chapter_text = chapter_file.read()\n",
    "        ir_new_chapter_texts[chapter_order(filename)] = [word for word in chapter_text.split() if word not in new_stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the model again for Topic detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_new_corpus_dict = corpora.Dictionary([values for key, values in mb_new_chapter_texts.items()])\n",
    "mb_new_corpus = [mb_new_corpus_dict.doc2bow(values) for key, values in mb_new_chapter_texts.items()]\n",
    "\n",
    "ir_new_corpus_dict = corpora.Dictionary([values for key, values in ir_new_chapter_texts.items()])\n",
    "ir_new_corpus = [ir_new_corpus_dict.doc2bow(values) for key, values in ir_new_chapter_texts.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.010*\"station\" + 0.009*\"feed\" + 0.007*\"humans\" + 0.007*\"security\" + 0.006*\"id\" + 0.005*\"human\"')\n",
      "(1, '0.007*\"humans\" + 0.006*\"mensah\" + 0.006*\"bot\" + 0.006*\"shuttle\" + 0.006*\"feed\" + 0.006*\"secunit\"')\n",
      "(2, '0.012*\"humans\" + 0.006*\"mensah\" + 0.005*\"human\" + 0.005*\"art\" + 0.005*\"feed\" + 0.005*\"security\"')\n",
      "(3, '0.010*\"amena\" + 0.008*\"feed\" + 0.007*\"art\" + 0.007*\"eletra\" + 0.005*\"ras\" + 0.005*\"humans\"')\n",
      "(4, '0.011*\"art\" + 0.010*\"humans\" + 0.006*\"ratthi\" + 0.005*\"feed\" + 0.005*\"secunit\" + 0.005*\"right\"')\n",
      "(5, '0.010*\"mensah\" + 0.008*\"feed\" + 0.006*\"back\" + 0.006*\"habitat\" + 0.005*\"us\" + 0.005*\"ratthi\"')\n",
      "(6, '0.000*\"humans\" + 0.000*\"feed\" + 0.000*\"going\" + 0.000*\"us\" + 0.000*\"mensah\" + 0.000*\"back\"')\n",
      "(7, '0.019*\"miki\" + 0.015*\"abene\" + 0.011*\"feed\" + 0.009*\"wilken\" + 0.006*\"shuttle\" + 0.006*\"gerth\"')\n",
      "(8, '0.007*\"us\" + 0.007*\"feed\" + 0.006*\"humans\" + 0.005*\"drones\" + 0.005*\"target\" + 0.005*\"two\"')\n",
      "(9, '0.007*\"humans\" + 0.007*\"gurathin\" + 0.006*\"security\" + 0.006*\"feed\" + 0.006*\"ratthi\" + 0.005*\"human\"')\n"
     ]
    }
   ],
   "source": [
    "mb_new_lda_model = LdaModel(corpus=mb_new_corpus, id2word=mb_new_corpus_dict, \n",
    "                            num_topics=10, random_state=42, passes=15)\n",
    "\n",
    "mb_new_topics = mb_new_lda_model.print_topics(num_words=6)\n",
    "\n",
    "for topic in mb_new_topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.011*\"lieutenant\" + 0.011*\"station\" + 0.011*\"captain\" + 0.008*\"sir\" + 0.005*\"ship\" + 0.005*\"fleet\"')\n",
      "(1, '0.013*\"station\" + 0.011*\"seivarden\" + 0.005*\"translator\" + 0.004*\"captain\" + 0.004*\"governor\" + 0.004*\"administrator\"')\n",
      "(2, '0.006*\"mercy\" + 0.006*\"anaander\" + 0.006*\"captain\" + 0.005*\"kalr\" + 0.005*\"seivarden\" + 0.005*\"gun\"')\n",
      "(3, '0.000*\"station\" + 0.000*\"lieutenant\" + 0.000*\"captain\" + 0.000*\"still\" + 0.000*\"fleet\" + 0.000*\"seivarden\"')\n",
      "(4, '0.014*\"lieutenant\" + 0.008*\"station\" + 0.007*\"captain\" + 0.006*\"sword\" + 0.006*\"tisarwat\" + 0.006*\"still\"')\n",
      "(5, '0.011*\"seivarden\" + 0.007*\"captain\" + 0.006*\"still\" + 0.006*\"ship\" + 0.005*\"tea\" + 0.005*\"even\"')\n",
      "(6, '0.024*\"lieutenant\" + 0.019*\"awn\" + 0.018*\"mianaai\" + 0.013*\"anaander\" + 0.011*\"lord\" + 0.007*\"radch\"')\n",
      "(7, '0.000*\"lieutenant\" + 0.000*\"seivarden\" + 0.000*\"captain\" + 0.000*\"even\" + 0.000*\"station\" + 0.000*\"ship\"')\n",
      "(8, '0.000*\"lieutenant\" + 0.000*\"captain\" + 0.000*\"station\" + 0.000*\"even\" + 0.000*\"seivarden\" + 0.000*\"ship\"')\n",
      "(9, '0.008*\"sphene\" + 0.006*\"official\" + 0.005*\"dariet\" + 0.004*\"var\" + 0.004*\"ghost\" + 0.004*\"invalid\"')\n"
     ]
    }
   ],
   "source": [
    "ir_new_lda_model = LdaModel(corpus=ir_new_corpus, id2word=ir_new_corpus_dict, num_topics=10, random_state=42, passes=15)\n",
    "\n",
    "ir_new_topics = ir_new_lda_model.print_topics(num_words=6)\n",
    "\n",
    "for topic in ir_new_topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "further removing \"didn't\" , \"like\", \"know\", \"one\", \"don't\". i don't know if \"one\" is relevant in the IR series but since its there frequently in both series, i'm assuming its not and removing from both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now it looks like we've taken out all the relevant topics/ characters. atleast i can attest that for the MB series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For series wise TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "humans\n",
      "feed\n",
      "art\n",
      "ratthi\n",
      "mensah\n",
      "human\n",
      "amena\n",
      "id\n",
      "station\n",
      "iris\n"
     ]
    }
   ],
   "source": [
    "mb_new_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "mb_new_tfidf_matrix = mb_new_vectorizer.fit_transform([\" \".join(values) for key, values in mb_new_chapter_texts.items()])\n",
    "\n",
    "\n",
    "mb_new_feature_names = mb_new_vectorizer.get_feature_names_out()\n",
    "\n",
    "mb_new_tfidf_sums = mb_new_tfidf_matrix.sum(axis=0)\n",
    "mb_new_sorted_indices = mb_new_tfidf_sums.argsort()[0, ::-1]\n",
    "\n",
    "mb_new_top_words = [mb_new_feature_names[i] for i in mb_new_sorted_indices[:10]]\n",
    "for word in mb_new_top_words[0][0][:10]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seivarden\n",
      "lieutenant\n",
      "captain\n",
      "station\n",
      "tisarwat\n",
      "fleet\n",
      "ship\n",
      "sir\n",
      "anaander\n",
      "translator\n"
     ]
    }
   ],
   "source": [
    "ir_new_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "ir_new_tfidf_matrix = ir_new_vectorizer.fit_transform([\" \".join(values) for key, values in ir_new_chapter_texts.items()])\n",
    "\n",
    "\n",
    "ir_new_feature_names = ir_new_vectorizer.get_feature_names_out()\n",
    "\n",
    "ir_new_tfidf_sums = ir_new_tfidf_matrix.sum(axis=0)\n",
    "ir_new_sorted_indices = ir_new_tfidf_sums.argsort()[0, ::-1]\n",
    "\n",
    "ir_new_top_words = [ir_new_feature_names[i] for i in ir_new_sorted_indices[:10]]\n",
    "for word in ir_new_top_words[0][0][:10]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This didn't change much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
