I JOLTED AWAKE FOUR hours later, when my automatic recharge cycle started. The transport said immediately, That was unnecessarily childish.

“What do you know about children?” I was even more angry now because it was right. The shutdown and the time I had spent inert would have driven off or distracted a human; the transport had just waited to resume the argument.

My crew complement includes teachers and students. I have accumulated many examples of childishness.

I just sat there, fuming. I wanted to go back to watching media, but I knew it would think it meant I was giving in, accepting the inevitable. For my entire existence, at least the parts I could remember, I had done nothing but accept the inevitable. I was tired of it.

We are friends now. I don’t understand why you won’t discuss your plans.

It was such an astonishing, infuriating statement. “We aren’t friends. The first thing you did when we were underway was threaten me,” I pointed out.

I needed to make certain you didn’t attempt to harm me.

I noticed it had said “attempt” and not “intend.” If it had cared anything about my intentions it wouldn’t have let me onboard in the first place. It had enjoyed showing me it was more powerful than a SecUnit.

Not that it was wrong about the “attempt.” While watching the episodes I had managed to do some analysis of it, using the schematics in its own public feed and the specs of similar transports available on the unsecured sections of its database. I had figured out twenty-seven different ways to render it inoperable and three to blow it up. But a mutually assured destruction scenario was not something I was interested in.

If I got through this intact, I needed to find a nicer, dumber transport for the next ride.

I hadn’t responded and I knew by now it couldn’t stand that. It said, I apologized. I still didn’t respond. It added, My crew always considers me trustworthy.

I shouldn’t have let it watch all those episodes of Worldhoppers. “I’m not your crew. I’m not a human. I’m a construct. Constructs and bots can’t trust each other.”

It was quiet for ten precious seconds, though I could tell from the spike in its feed activity it was doing something. I realized it must be searching its databases, looking for a way to refute my statement. Then it said, Why not?

I had spent so much time pretending to be patient with humans asking stupid questions. I should have more self-control than this. “Because we both have to follow human orders. A human could tell you to purge my memory. A human could tell me to destroy your systems.”

I thought it would argue that I couldn’t possibly hurt it, which would derail the whole conversation.

But it said, There are no humans here now.

I realized I had been trapped into this conversational dead end, with the transport pretending to need this explained in order to get me to articulate it to myself. I didn’t know who I was more annoyed at, myself or it. No, I was definitely more annoyed at it.

I sat there for a while, wanting to go back to the media, any media, rather than think about this. I could feel it in the feed, waiting, watching me with all its attention except for the miniscule amount of awareness it needed to keep itself on course.

Did it really matter if it knew? Was I afraid knowing would change its opinion of me? (As far as I could tell, its opinion was already pretty low.) Did I really care what an asshole research transport thought about me?

I shouldn’t have asked myself that question. I felt a wave of non-caring about to come over me, and I knew I couldn’t let it. If I was going to follow my plan, such as it was, I needed to care. If I let myself not care, then there was no telling where I’d end up. Riding dumb transports watching media until somebody caught me and sold me back to the company, probably, or killed me for my inorganic parts.

I said, “At some point approximately 35,000 hours ago, I was assigned to a contract on RaviHyral Mining Facility Q Station. During that assignment, I went rogue and killed a large number of my clients. My memory of the incident was partially purged.” SecUnit memory purges are always partial, due to the organic parts inside our heads. The purge can’t wipe memory from organic neural tissue. “I need to know if the incident occurred due to a catastrophic failure of my governor module. That’s what I think happened. But I need to know for sure.” I hesitated, but what the hell, it already knew everything else. “I need to know if I hacked my governor module in order to cause the incident.”

I don’t know what I expected. I knew ART (aka Asshole Research Transport) had a deeper attachment to its crew than SecUnits had for clients. If it didn’t feel that way about the humans it carried and worked with, then it wouldn’t have gotten so upset whenever anything happened to the characters on Worldhoppers. I wouldn’t have had to filter out all the based-on-a-true-story shows where human crews got hurt. I knew what it felt, because I felt that way about Mensah and PreservationAux.

It said, Why was your memory of the incident purged?

That wasn’t the question I was expecting. “Because SecUnits are expensive and the company didn’t want to lose any more money on me than it already had.” I wanted to fidget. I wanted to say something so offensive to it that it would leave me alone. I really wanted to stop thinking about this and watch Sanctuary Moon. “Either I killed them due to a malfunction and then hacked the governor module, or I hacked the governor module so I could kill them,” I said. “Those are the only two possibilities.”

Are all constructs so illogical? said the Asshole Research Transport with the immense processing capability whose metaphorical hand I had had to hold because it had become emotionally compromised by a fictional media serial. Before I could say that, it added, Those are not the first two possibilities to consider.

I had no idea what it meant. “All right, what are the first two possibilities to consider?”

That it either happened, or it didn’t.